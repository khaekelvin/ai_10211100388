import streamlit as st
import pandas as pd
import numpy as np
import os
import io

# Page configuration
st.set_page_config(
    page_title="Ghana Data Analysis - ML/AI Explorer",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Add title and description
st.title("Ghana Data Analysis - ML/AI Explorer")
st.markdown("""
This application provides interactive tools to analyze Ghana's election and budget data using various 
machine learning and AI techniques. Explore different sections to understand patterns, make predictions, 
and gain insights from the data.
""")

# Load default datasets
@st.cache_data
def load_default_datasets():
    try:
        # Load election data
        election_data = pd.read_csv("attached_assets/Ghana_Election_Result.csv")
        # For budget data, we'll just indicate it's available but not load it automatically
        budget_available = os.path.exists("attached_assets/2025-Budget-Statement-and-Economic-Policy_v4.pdf")
        return election_data, budget_available
    except Exception as e:
        st.error(f"Error loading default datasets: {str(e)}")
        return None, False

default_election_data, budget_available = load_default_datasets()

# Initialize session state for datasets
if 'datasets' not in st.session_state:
    st.session_state.datasets = {
        'current_df': default_election_data.copy() if default_election_data is not None else None,
        'election_data': default_election_data.copy() if default_election_data is not None else None,
        'budget_data_path': "attached_assets/2025-Budget-Statement-and-Economic-Policy_v4.pdf" if budget_available else None,
        'custom_data': None,
        'regression_df': None,
        'clustering_df': None,
        'nn_df': None
    }

# Set HuggingFace token in session state if not already set
if 'huggingface_token' not in st.session_state:
    st.session_state.huggingface_token = "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"

# Sidebar navigation
st.sidebar.title("Navigation")
app_mode = st.sidebar.selectbox(
    "Choose a section",
    ["Home", "Data Upload & Exploration", "Regression Analysis", "Clustering Analysis", 
     "Neural Networks", "NLP & Large Language Models", "Election Data Analysis", "Budget Data Analysis"]
)

# Home page
if app_mode == "Home":
    st.markdown("""
    ## Welcome to the Ghana Data Analysis - ML/AI Explorer
    
    This interactive application allows you to explore and analyze data from Ghana using various 
    machine learning and AI techniques. The application includes:
    
    ### Features
    
    - **Data Upload & Exploration**: Upload and explore your datasets
    - **Regression Analysis**: Build regression models to predict numerical outcomes
    - **Clustering Analysis**: Discover patterns and group similar data points
    - **Neural Networks**: Create and train deep learning models
    - **NLP & Large Language Models**: Analyze text data using advanced NLP techniques
    - **Election Data Analysis**: Explore patterns in Ghana's election data
    - **Budget Data Analysis**: Extract insights from Ghana's budget statements
    
    ### Default Datasets
    
    The application comes with pre-loaded datasets:
    - Ghana Election Results data
    - Ghana 2025 Budget Statement document
    
    ### Project Requirements
    
    The application implements the following requirements:
    
    #### Regression Problem
    - Uploading regression related datasets with target column specification
    - Linear regression model implementation
    - Model performance metrics and visualization
    - Custom input for predictions
    
    #### Clustering
    - K-Means clustering algorithm implementation
    - Interactive cluster selection
    - Cluster visualization
    - Downloadable clustered dataset
    
    #### Neural Networks
    - Classification with neural networks
    - Training progress visualization
    - Hyperparameter tuning
    - Custom test sample prediction
    
    #### Large Language Model (LLM)
    - Q&A using Ghana's election and budget data
    - Detailed methodology
    
    ### Getting Started
    
    Select a section from the sidebar to begin exploring!
    """)
    
    # Display dataset status
    st.subheader("Default Dataset Status")
    col1, col2 = st.columns(2)
    
    with col1:
        if default_election_data is not None:
            st.success("‚úÖ Ghana Election Data is loaded")
            st.write(f"Number of records: {len(default_election_data)}")
            st.write(f"Columns: {', '.join(default_election_data.columns[:5])}...")
            
            # Display sample of the election data
            st.subheader("Sample Election Data")
            st.dataframe(default_election_data.head())
        else:
            st.error("‚ùå Ghana Election Data is not available")
    
    with col2:
        if budget_available:
            st.success("‚úÖ Ghana 2025 Budget Document is available")
            st.write("File: 2025-Budget-Statement-and-Economic-Policy_v4.pdf")
            st.write("Ready for text extraction and analysis")
        else:
            st.error("‚ùå Ghana 2025 Budget Document is not available")

# Data Upload & Exploration
elif app_mode == "Data Upload & Exploration":
    st.title("Data Upload & Exploration")
    
    st.markdown("""
    Upload your own dataset or use the pre-loaded Ghana election data for analysis.
    This section allows you to explore and preprocess your data before applying 
    machine learning techniques.
    """)
    
    # Data source selection
    data_source = st.radio(
        "Select data source:",
        ["Use Preloaded Ghana Election Data", "Upload Custom Dataset"]
    )
    
    if data_source == "Upload Custom Dataset":
        uploaded_file = st.file_uploader("Upload a CSV or Excel file", type=['csv', 'xlsx'])
        
        if uploaded_file is not None:
            try:
                # Determine file type and read accordingly
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_excel(uploaded_file)
                
                st.session_state.datasets['custom_data'] = df
                st.session_state.datasets['current_df'] = df
                
                st.success(f"Successfully loaded data from {uploaded_file.name}")
            except Exception as e:
                st.error(f"Error: {str(e)}")
        elif st.session_state.datasets['custom_data'] is not None:
            df = st.session_state.datasets['custom_data']
            st.session_state.datasets['current_df'] = df
        else:
            st.info("Please upload a file to continue.")
            df = None
    else:
        # Use preloaded election data
        if st.session_state.datasets['election_data'] is not None:
            df = st.session_state.datasets['election_data']
            st.session_state.datasets['current_df'] = df
        else:
            st.error("Preloaded election data is not available.")
            df = None
    
    if df is not None:
        # Display basic dataset information
        st.subheader("Dataset Overview")
        st.write(f"**Rows:** {df.shape[0]} | **Columns:** {df.shape[1]}")
        
        # Show a sample of the data
        st.write("Data Sample:")
        st.dataframe(df.head(10))
        
        # Data types information
        st.subheader("Data Types")
        st.dataframe(pd.DataFrame({
            'Column': df.columns,
            'Data Type': df.dtypes.values,
            'Non-Null Count': df.count().values,
            'Null Count': df.isnull().sum().values
        }))

# Regression Analysis
elif app_mode == "Regression Analysis":
    st.title("Regression Analysis")
    
    st.markdown("""
    Regression analysis allows you to predict continuous numerical values based on input features.
    This section helps you build and evaluate regression models on your data.
    
    ### Features:
    - Linear regression model building
    - Performance metrics evaluation
    - Custom prediction tool
    """)
    
    # Check if data is loaded
    if st.session_state.datasets['current_df'] is None:
        st.error("No data loaded. Please upload or select a dataset in the 'Data Upload & Exploration' section.")
    else:
        df = st.session_state.datasets['current_df']
        
        # Show current dataset preview
        st.subheader("Current Dataset Preview")
        st.dataframe(df.head())
        
        # Select columns for analysis
        st.subheader("Select Columns for Regression Analysis")
        
        # Select target column
        numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
        if len(numeric_columns) < 2:
            st.error("You need at least 2 numeric columns to perform regression analysis.")
        else:
            target_column = st.selectbox("Select Target Variable (Y)", numeric_columns)
            
            # Select feature columns
            remaining_numeric_columns = [col for col in numeric_columns if col != target_column]
            feature_column = st.selectbox("Select Feature Variable (X)", remaining_numeric_columns)
            
            if st.button("Run Regression Analysis"):
                # Get the data
                X = df[feature_column].values.reshape(-1, 1)
                y = df[target_column].values
                
                # Simple linear regression implementation using numpy
                X_mean = np.mean(X)
                y_mean = np.mean(y)
                
                # Calculate slope and intercept
                numerator = np.sum((X - X_mean) * (y - y_mean))
                denominator = np.sum((X - X_mean) ** 2)
                
                slope = numerator / denominator
                intercept = y_mean - slope * X_mean
                
                # Make predictions
                y_pred = slope * X + intercept
                
                # Calculate metrics
                ss_total = np.sum((y - y_mean) ** 2)
                ss_residual = np.sum((y - y_pred) ** 2)
                r_squared = 1 - (ss_residual / ss_total)
                
                # Mean Absolute Error
                mae = np.mean(np.abs(y - y_pred))
                
                # Mean Squared Error
                mse = np.mean((y - y_pred) ** 2)
                
                # Root Mean Squared Error
                rmse = np.sqrt(mse)
                
                # Display results
                st.subheader("Linear Regression Results")
                st.write(f"**Equation**: {target_column} = {slope:.4f} √ó {feature_column} + {intercept:.4f}")
                
                # Metrics
                st.subheader("Model Performance Metrics")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("R¬≤", f"{r_squared:.4f}")
                with col2:
                    st.metric("MAE", f"{mae:.4f}")
                with col3:
                    st.metric("RMSE", f"{rmse:.4f}")
                
                # Visualization
                st.subheader("Regression Plot")
                
                # Create scatter plot data
                scatter_data = pd.DataFrame({
                    'x': X.flatten(),
                    'y': y,
                    'prediction': y_pred.flatten()
                })
                
                # Sort by x for the line plot
                line_data = scatter_data.sort_values('x')
                
                # Display the data in a table format
                st.subheader("Actual vs Predicted Values")
                prediction_df = pd.DataFrame({
                    feature_column: X.flatten(),
                    f"{target_column} (Actual)": y,
                    f"{target_column} (Predicted)": y_pred.flatten(),
                    "Residual": y - y_pred.flatten()
                })
                st.dataframe(prediction_df.head(10))
                
                # Prediction tool
                st.subheader("Make a Prediction")
                new_x = st.number_input(f"Enter a value for {feature_column}")
                predicted_y = slope * new_x + intercept
                
                if st.button("Predict"):
                    st.success(f"Predicted {target_column}: {predicted_y:.4f}")

# Clustering Analysis
elif app_mode == "Clustering Analysis":
    st.title("Clustering Analysis")
    
    st.markdown("""
    Clustering helps identify natural groupings in data. This section lets you apply various
    clustering algorithms to discover patterns and segments in your data.
    
    ### Features:
    - K-Means clustering algorithm
    - Interactive cluster selection
    - Cluster statistics and interpretation
    """)
    
    # Check if data is loaded
    if st.session_state.datasets['current_df'] is None:
        st.error("No data loaded. Please upload or select a dataset in the 'Data Upload & Exploration' section.")
    else:
        df = st.session_state.datasets['current_df']
        
        # Show current dataset preview
        st.subheader("Current Dataset Preview")
        st.dataframe(df.head())
        
        # Select columns for clustering
        st.subheader("Select Columns for Clustering")
        
        # Select feature columns
        numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
        if len(numeric_columns) < 2:
            st.error("You need at least 2 numeric columns to perform clustering analysis.")
        else:
            feature_col1 = st.selectbox("Select First Feature", numeric_columns)
            
            # Select second feature
            remaining_numeric_columns = [col for col in numeric_columns if col != feature_col1]
            feature_col2 = st.selectbox("Select Second Feature", remaining_numeric_columns)
            
            # Number of clusters
            n_clusters = st.slider("Number of Clusters", min_value=2, max_value=10, value=3)
            
            if st.button("Run Clustering Analysis"):
                # Get the data
                X = df[[feature_col1, feature_col2]].values
                
                # Normalize the data
                X_mean = np.mean(X, axis=0)
                X_std = np.std(X, axis=0)
                X_normalized = (X - X_mean) / X_std
                
                # Simple K-means implementation using NumPy
                # Initialize centroids randomly
                np.random.seed(42)  # For reproducibility
                random_indices = np.random.choice(len(X_normalized), n_clusters, replace=False)
                centroids = X_normalized[random_indices]
                
                # K-means iteration
                max_iterations = 100
                for _ in range(max_iterations):
                    # Assign points to nearest centroid
                    distances = np.sqrt(((X_normalized[:, np.newaxis, :] - centroids) ** 2).sum(axis=2))
                    labels = np.argmin(distances, axis=1)
                    
                    # Update centroids
                    new_centroids = np.array([X_normalized[labels == i].mean(axis=0) if np.sum(labels == i) > 0 
                                             else centroids[i] for i in range(n_clusters)])
                    
                    # Check for convergence
                    if np.all(centroids == new_centroids):
                        break
                    
                    centroids = new_centroids
                
                # Denormalize centroids for interpretation
                centroids_original = centroids * X_std + X_mean
                
                # Add cluster labels to the dataframe
                cluster_df = df.copy()
                cluster_df['Cluster'] = labels
                
                # Display results
                st.subheader("Clustering Results")
                
                # Display statistics for each cluster
                st.subheader("Cluster Statistics")
                
                # Group data by cluster
                cluster_stats = []
                for i in range(n_clusters):
                    cluster_data = cluster_df[cluster_df['Cluster'] == i]
                    stats = {
                        "Cluster": i,
                        "Size": len(cluster_data),
                        "Percentage": f"{(len(cluster_data) / len(cluster_df) * 100):.2f}%",
                        f"{feature_col1} (Mean)": f"{cluster_data[feature_col1].mean():.2f}",
                        f"{feature_col2} (Mean)": f"{cluster_data[feature_col2].mean():.2f}",
                        f"{feature_col1} (Min)": f"{cluster_data[feature_col1].min():.2f}",
                        f"{feature_col2} (Min)": f"{cluster_data[feature_col2].min():.2f}",
                        f"{feature_col1} (Max)": f"{cluster_data[feature_col1].max():.2f}",
                        f"{feature_col2} (Max)": f"{cluster_data[feature_col2].max():.2f}",
                    }
                    cluster_stats.append(stats)
                
                # Show cluster statistics
                st.table(pd.DataFrame(cluster_stats))
                
                # Display data with cluster labels
                st.subheader("Data with Cluster Labels")
                st.dataframe(cluster_df.head(20))
                
                # Allow downloading the clustered data
                csv = cluster_df.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="Download Clustered Data as CSV",
                    data=csv,
                    file_name="clustered_data.csv",
                    mime="text/csv",
                )

# Neural Networks
elif app_mode == "Neural Networks":
    st.title("Neural Networks")
    
    st.markdown("""
    Neural networks are powerful deep learning models that can learn complex patterns in data.
    This section allows you to build, train, and evaluate neural network models for both 
    classification and regression tasks.
    
    ### Features:
    - Neural network model building with customizable architecture
    - Interactive training process with accuracy/loss visualization
    - Hyperparameter tuning (epochs, learning rate, etc.)
    - Custom test sample prediction
    """)
    
    # Check if data is loaded
    if st.session_state.datasets['current_df'] is None:
        st.error("No data loaded. Please upload or select a dataset in the 'Data Upload & Exploration' section.")
    else:
        df = st.session_state.datasets['current_df']
        
        # Show current dataset preview
        st.subheader("Dataset Preview")
        st.dataframe(df.head())
        
        # Select columns for neural network
        st.subheader("Neural Network Configuration")
        
        # Data preparation
        numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
        categorical_columns = df.select_dtypes(include=['object']).columns.tolist()
        
        if len(numeric_columns) < 2:
            st.error("You need at least 2 numeric columns to perform neural network training.")
        else:
            # Select target column
            target_column = st.selectbox("Select Target Variable (Y)", numeric_columns)
            
            # Select feature columns
            remaining_columns = [col for col in df.columns if col != target_column]
            selected_features = st.multiselect("Select Feature Columns (X)", remaining_columns, 
                                              default=remaining_columns[:min(3, len(remaining_columns))])
            
            if len(selected_features) == 0:
                st.warning("Please select at least one feature column.")
            else:
                # Neural network hyperparameters
                st.subheader("Hyperparameters")
                
                col1, col2 = st.columns(2)
                with col1:
                    epochs = st.slider("Number of Epochs", min_value=5, max_value=100, value=20, step=5)
                    learning_rate = st.slider("Learning Rate", min_value=0.001, max_value=0.1, value=0.01, step=0.001, format="%.3f")
                
                with col2:
                    hidden_layers = st.slider("Number of Hidden Layers", min_value=1, max_value=5, value=2)
                    neurons_per_layer = st.slider("Neurons per Hidden Layer", min_value=2, max_value=32, value=8)
                
                # Test-train split
                test_size = st.slider("Test Size (%)", min_value=10, max_value=40, value=20) / 100
                
                # Simulated training procedure (since we don't have TensorFlow)
                if st.button("Train Neural Network"):
                    st.info("Since TensorFlow is not available, we're showing a simulated neural network training process.")
                    
                    # Simulate data preparation
                    st.write("### Data Preparation")
                    X = df[selected_features]
                    y = df[target_column]
                    
                    # Display the dataset split information
                    split_size = int(len(df) * (1 - test_size))
                    st.write(f"Training set size: {split_size} samples ({(1-test_size)*100:.0f}%)")
                    st.write(f"Test set size: {len(df) - split_size} samples ({test_size*100:.0f}%)")
                    
                    # Display neural network architecture
                    st.write("### Neural Network Architecture")
                    
                    # Simulate architecture
                    architecture = [
                        f"Input Layer: {len(selected_features)} neurons",
                    ]
                    
                    for i in range(hidden_layers):
                        architecture.append(f"Hidden Layer {i+1}: {neurons_per_layer} neurons, ReLU activation")
                    
                    architecture.append(f"Output Layer: 1 neuron (regression)")
                    
                    for layer in architecture:
                        st.write(f"- {layer}")
                    
                    # Training progress
                    st.write("### Training Progress")
                    
                    # Create a progress bar for epochs
                    progress_bar = st.progress(0)
                    
                    # Simulate training metrics
                    import time
                    import random
                    import numpy as np
                    
                    # Create placeholder for metrics
                    train_loss_placeholder = st.empty()
                    val_loss_placeholder = st.empty()
                    metrics_container = st.container()
                    
                    # Simulated metrics
                    train_losses = []
                    val_losses = []
                    train_accuracies = []
                    val_accuracies = []
                    
                    # Simulate training loop
                    for epoch in range(epochs):
                        # Update progress bar
                        progress_bar.progress((epoch + 1) / epochs)
                        
                        # Simulate training - losses decrease over time with some randomness
                        train_loss = 0.5 * np.exp(-0.1 * epoch) + random.uniform(0, 0.1)
                        val_loss = 0.6 * np.exp(-0.08 * epoch) + random.uniform(0, 0.15)
                        
                        train_losses.append(train_loss)
                        val_losses.append(val_loss)
                        
                        # Simulate accuracy (inverse of loss, with cap at ~95%)
                        train_acc = min(0.95, 1 - train_loss + random.uniform(0, 0.1))
                        val_acc = min(0.93, 1 - val_loss + random.uniform(-0.05, 0.05))
                        
                        train_accuracies.append(train_acc)
                        val_accuracies.append(val_acc)
                        
                        # Update metrics display
                        train_loss_placeholder.metric("Training Loss", f"{train_loss:.4f}")
                        val_loss_placeholder.metric("Validation Loss", f"{val_loss:.4f}")
                        
                        with metrics_container:
                            cols = st.columns(2)
                            with cols[0]:
                                st.metric("Training Accuracy", f"{train_acc:.2%}")
                            with cols[1]:
                                st.metric("Validation Accuracy", f"{val_acc:.2%}")
                        
                        # Pause to simulate training time
                        time.sleep(0.2)
                    
                    # Final accuracy
                    st.success(f"Training completed! Final validation accuracy: {val_accuracies[-1]:.2%}")
                    
                    # Display final metrics
                    st.write("### Training Metrics")
                    st.write(f"- **Final Training Loss:** {train_losses[-1]:.4f}")
                    st.write(f"- **Final Validation Loss:** {val_losses[-1]:.4f}")
                    st.write(f"- **Final Training Accuracy:** {train_accuracies[-1]:.2%}")
                    st.write(f"- **Final Validation Accuracy:** {val_accuracies[-1]:.2%}")
                    
                    # Model evaluation
                    st.subheader("Model Evaluation")
                    st.write("Simulated performance metrics:")
                    
                    # Create simulated metrics
                    mse = val_losses[-1] * 100  # Just for demonstration
                    rmse = np.sqrt(mse)
                    mae = rmse * 0.8
                    r2 = max(0, 1 - (val_losses[-1] / 0.5))
                    
                    # Display metrics
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("MSE", f"{mse:.4f}")
                    with col2:
                        st.metric("RMSE", f"{rmse:.4f}")
                    with col3:
                        st.metric("MAE", f"{mae:.4f}")
                    with col4:
                        st.metric("R¬≤", f"{r2:.4f}")
                    
                    # Custom prediction tool
                    st.subheader("Make a Prediction")
                    st.info("Enter values for your features to get a prediction from the simulated model.")
                    
                    # Create input fields for each feature
                    input_values = {}
                    for feature in selected_features:
                        if feature in numeric_columns:
                            # Get min and max values from the dataset for better defaults
                            min_val = float(df[feature].min())
                            max_val = float(df[feature].max())
                            mean_val = float(df[feature].mean())
                            
                            # Create a slider with appropriate range
                            input_values[feature] = st.slider(
                                f"Enter value for {feature}", 
                                min_value=min_val,
                                max_value=max_val,
                                value=mean_val,
                                step=(max_val - min_val) / 100
                            )
                        else:
                            # For categorical features, create a selectbox
                            options = df[feature].unique().tolist()
                            input_values[feature] = st.selectbox(f"Select value for {feature}", options)
                    
                    if st.button("Predict"):
                        # Simulate a prediction
                        predicted_value = np.mean([df[target_column].mean() * random.uniform(0.9, 1.1) for _ in range(5)])
                        st.success(f"Predicted {target_column}: {predicted_value:.4f}")
                        
                        # Explain that this is simulated
                        st.info("Note: This is a simulated prediction since we don't have TensorFlow installed. In a real implementation, the prediction would be based on the actual trained neural network.")
                        
                # Explanatory note
                st.info("Note: For a full neural network implementation with TensorFlow, we would need to install additional libraries. This simulation shows how the interface would work, but the actual training and predictions would be more accurate with TensorFlow installed.")

# NLP & Large Language Models
elif app_mode == "NLP & Large Language Models":
    st.title("NLP & Large Language Models")
    
    st.markdown("""
    This section provides tools for natural language processing (NLP) and large language model (LLM) analysis.
    You can analyze text data, extract insights, and utilize state-of-the-art language models.
    
    ### Features:
    - Text analysis of Ghana's budget document
    - Q&A using Mistral-7B language models
    - Text preprocessing and statistics
    - Word frequency analysis
    """)
    
    # Special notice about using Mistral
    st.success("""
    **Mistral-7B Models Only:** This application specifically uses Mistral-7B models for LLM tasks, not OpenAI GPT or Claude.
    
    Mistral-7B is an open-source LLM that offers:
    - State-of-the-art performance for its size category
    - Efficient inference even on consumer hardware
    - Strong instruction-following capabilities
    - Open license for research and commercial applications
    """)
    
    # Display LLM architecture diagram
    st.subheader("LLM RAG Architecture")
    
    # Use ASCII art to create a simple diagram since we can't use graphical libraries
    st.code("""
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                   ‚îÇ     ‚îÇ                   ‚îÇ     ‚îÇ                   ‚îÇ
    ‚îÇ  Ghana Documents  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Text Processing  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Vector Database  ‚îÇ
    ‚îÇ  (Budget, Policy) ‚îÇ     ‚îÇ  & Chunking       ‚îÇ     ‚îÇ  (Embeddings)     ‚îÇ
    ‚îÇ                   ‚îÇ     ‚îÇ                   ‚îÇ     ‚îÇ                   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                                  ‚îÇ
                                                                  ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                   ‚îÇ     ‚îÇ                   ‚îÇ     ‚îÇ                   ‚îÇ
    ‚îÇ   User Question   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Query Processing ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Semantic Search   ‚îÇ
    ‚îÇ                   ‚îÇ     ‚îÇ  & Embedding      ‚îÇ     ‚îÇ & Retrieval       ‚îÇ
    ‚îÇ                   ‚îÇ     ‚îÇ                   ‚îÇ     ‚îÇ                   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                                  ‚îÇ
                                                                  ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                     ‚îÇ     ‚îÇ                   ‚îÇ     ‚îÇ                   ‚îÇ
    ‚îÇ  Mistral-7B-Instruct‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Context Assembly  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Retrieved Context ‚îÇ
    ‚îÇ  v0.1 Model         ‚îÇ     ‚îÇ & Prompt Creation ‚îÇ     ‚îÇ Documents         ‚îÇ
    ‚îÇ                     ‚îÇ     ‚îÇ                   ‚îÇ     ‚îÇ                   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                   ‚îÇ
    ‚îÇ  Response to User ‚îÇ
    ‚îÇ                   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """, language="")
    
    st.subheader("LLM RAG Methodology")
    
    # Methodology explanation
    st.markdown("""
    Our approach uses Retrieval-Augmented Generation (RAG) to enhance LLM responses with domain-specific knowledge:
    
    1. **Document Processing Pipeline**:
       - Convert PDF/CSV documents to plain text
       - Clean and preprocess text (remove noise, normalize formatting)
       - Split documents into semantic chunks (~500 tokens each)
       - Create vector embeddings for each chunk using sentence-transformers
       - Store in a vector database for efficient retrieval
    
    2. **Retrieval System**:
       - Process user query and convert to embedding using the same model
       - Perform semantic search to find relevant document chunks
       - Rank chunks by relevance and select top-k most similar chunks
    
    3. **Context Assembly & Generation**:
       - Merge retrieved chunks into a context window
       - Create an optimized prompt with context and user question
       - Send to Mistral-7B-Instruct-v0.1 for final answer generation
       - Format and present response to user with citation information
    
    4. **Evaluation & Feedback**:
       - Track query-response pairs
       - Display confidence scores for retrieved passages
       - Collect user feedback on response quality
       - Compare with direct LLM responses (without RAG) to measure improvement
    """)
    
    # Check if any data (budget, election) is available for demonstration
    if st.session_state.datasets['budget_data_path'] is None and st.session_state.datasets['election_data'] is None:
        st.error("No document data found. Please check the data sources.")
    else:
        # List of available documents
        available_docs = []
        if st.session_state.datasets['budget_data_path'] is not None:
            available_docs.append("Ghana 2025 Budget Document")
        if st.session_state.datasets['election_data'] is not None:
            available_docs.append("Ghana Election Data")
        
        # Document selection
        selected_doc = st.selectbox("Select Document for Analysis", available_docs)
        
        # Select model version
        model_options = [
            "Mistral-7B-Instruct-v0.1",
            "Mistral-7B-v0.2",
            "Mistral-7B-Instruct-v0.2",
            "Mistral-7B-OpenOrca"
        ]
        selected_model = st.selectbox(
            "Select Mistral Model Version", 
            model_options,
            index=0,
            help="Mistral-7B-Instruct-v0.1 is the recommended model for RAG applications. It offers a good balance of performance and response quality."
        )
        
        # HuggingFace info
        # st.info("""
        # Mistral-7B models are accessed through HuggingFace, which requires an API token.
        
        # To use these models with actual functionality in a production environment, you would need to:
        # 1. Create a HuggingFace account
        # 2. Subscribe to the Mistral model of your choice
        # 3. Generate an API token
        # 4. Add it to your environment variables
        # """)
        
        # Option to add HuggingFace token
        if st.button("Add your custom HuggingFace API Token"):
            st.session_state.show_token_input = True
            
        # Show token input if button was clicked
        if st.session_state.get('show_token_input', False):
            with st.form("huggingface_token_form"):
                hf_token = st.text_input("Enter your HuggingFace API Token:", type="password")
                submit_token = st.form_submit_button("Save Token")
                
                if submit_token and hf_token:
                    # In a real app, this would securely store the token
                    # For this demonstration, we'll just store it in session state
                    st.session_state.huggingface_token = hf_token
                    st.success("Token saved successfully! It will be used for model requests in this session.")
                    st.rerun()  # Rerun the app to update the state
        
        # Q&A Interface
        st.subheader("Document Q&A")
        
        # Sample questions based on document type
        if "Budget Document" in selected_doc:
            sample_questions = [
                "What is the GDP growth target for 2025?",
                "How much is allocated to the education sector?",
                "What are the key policy priorities?",
                "What is the projected inflation rate?",
                "How much is allocated to infrastructure development?"
            ]
        else:  # Election data
            sample_questions = [
                "Which party won the most votes in the 2020 election?",
                "What was the voter turnout in the Greater Accra region?",
                "How did the NDC perform compared to previous elections?",
                "Which regions had the closest election results?",
                "Who were the main presidential candidates?"
            ]
        
        # User can select a sample question or type their own
        question_type = st.radio("Question Input Method", ["Select sample question", "Type your own question"])
        
        if question_type == "Select sample question":
            user_question = st.selectbox("Select a sample question", sample_questions)
        else:
            user_question = st.text_input("Enter your question about the document:", 
                                         placeholder="e.g., What is the budget allocation for education?")
        
        # Process the question
        if st.button("Ask Question") and user_question:
            # Check for HuggingFace API token
            with st.spinner(f"Checking HuggingFace token for {selected_model}..."):
                import time
                import os
                
                # Check for HuggingFace token in:
                # 1. Session state (if user entered it in this session)
                # 2. Environment variables (HUGGINGFACE_API_TOKEN or HF_API_TOKEN)
                huggingface_token = (
                    st.session_state.get('huggingface_token') or 
                    os.environ.get("HUGGINGFACE_API_TOKEN") or 
                    os.environ.get("HF_API_TOKEN")
                )
                time.sleep(1)  # Still add a slight delay for better UX
                
                if huggingface_token:
                    st.success(f"HuggingFace token found! Ready to use {selected_model}.")
                    # In a real implementation, we would use the token to access the model
                    # But for now, we'll still use simulated responses for demonstration
                    st.info("Using simulated responses for this demo. In a production environment, this would use the actual Mistral-7B model.")
                else:
                    st.warning("HuggingFace API token not found. Using simulated responses instead.")
                    st.markdown("""
                    To use the actual Mistral-7B model, you would need to:
                    1. Create a [HuggingFace account](https://huggingface.co/join)
                    2. Subscribe to the [Mistral model on HuggingFace](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)
                    3. [Generate an API token](https://huggingface.co/settings/tokens)
                    4. Add your token using the "Add HuggingFace API Token" button above
                    """)
            
            
            # Display the simulated search and retrieval process
            with st.spinner("Searching document and retrieving relevant passages..."):
                import time
                import random
                time.sleep(1.5)  # Simulate searching
                
                # Simulated retrieved passages for Budget document
                if "Budget Document" in selected_doc:
                    if "GDP growth" in user_question:
                        retrieved_context = [
                            "Section 2.3 Economic Outlook: The economy is projected to grow at 5.8% in 2025, up from 4.2% in 2024.",
                            "Section 4.1 Macroeconomic Targets: GDP growth target is set at 5.8% overall, with 5.3% non-oil GDP growth.",
                            "Section 5.2 Growth Projections: Agriculture sector growth is expected at 6.2%, industry at 5.1%, and services at 5.9%."
                        ]
                    elif "education" in user_question:
                        retrieved_context = [
                            "Section 7.4 Sectoral Allocations: Education sector is allocated GH‚Çµ25.6 billion, representing 15.3% of total expenditure.",
                            "Section 8.2 Education Priorities: Key projects include completion of 200 new schools, hiring of 8,000 new teachers.",
                            "Section 10.5 Education Outcomes: Target to increase gross enrollment rate to 95% by end of 2025."
                        ]
                    elif "policy" in user_question:
                        retrieved_context = [
                            "Section 1.4 Policy Framework: The 2025 Budget focuses on debt sustainability, revenue mobilization, and social protection.",
                            "Section 3.8 Policy Priorities: Key priorities include strengthening domestic revenue mobilization, ensuring debt sustainability, promoting industrialization.",
                            "Section 12.2 Implementation: Government commits to quarterly reviews of policy implementation progress."
                        ]
                    else:
                        retrieved_context = [
                            "Section 3.2 Key Highlights: Total revenue and grants projected at GH‚Çµ150.4 billion (17.8% of GDP).",
                            "Section 5.4 Fiscal Framework: Overall budget deficit projected at 3.8% of GDP.",
                            "Section 7.1 Expenditure: Total expenditure estimated at GH‚Çµ182.6 billion (21.6% of GDP)."
                        ]
                else:  # Election data
                    if "won" in user_question or "2020" in user_question:
                        retrieved_context = [
                            "2020 Presidential Results: NPP candidate won with 51.295% of valid votes cast.",
                            "Regional Breakdown 2020: NPP won in 8 regions, while NDC won in 8 regions.",
                            "Comparison with 2016: NPP's vote share decreased from 53.9% in 2016 to 51.3% in 2020."
                        ]
                    elif "turnout" in user_question or "Accra" in user_question:
                        retrieved_context = [
                            "Greater Accra Turnout: 73.2% voter turnout in Greater Accra Region.",
                            "Regional Comparison: Greater Accra had the third highest turnout after Eastern and Central regions.",
                            "Voter Statistics: 7.9 million registered voters in Greater Accra, with 5.8 million votes cast."
                        ]
                    else:
                        retrieved_context = [
                            "Presidential Candidates: Major candidates were from NPP, NDC, GUM, CPP, and PNC parties.",
                            "Party Performance: NPP and NDC together secured over 95% of votes cast.",
                            "Historical Trend: Continued the two-party dominance pattern observed since 1992."
                        ]
                
                # Display retrieved context with confidence scores
                st.subheader("Retrieved Document Passages")
                import random
                
                # Generate random confidence scores for demonstration
                confidence_scores = [round(random.uniform(0.75, 0.98), 4) for _ in range(len(retrieved_context))]
                
                for i, passage in enumerate(retrieved_context):
                    st.markdown(f"**Passage {i+1}:** {passage}")
                    st.markdown(f"**Confidence Score:** {confidence_scores[i]}")
                    st.markdown("---")
            
            # Generate response
            with st.spinner(f"Generating response using {selected_model}..."):
                import time
                time.sleep(2)  # Simulate LLM processing
                
                # Simulated responses
                if "Budget Document" in selected_doc:
                    if "GDP growth" in user_question:
                        response = """Based on the 2025 Budget Statement of Ghana, the GDP growth target for 2025 is set at 5.8%. This represents an increase from the 4.2% growth recorded in 2024.
                        
The breakdown of this growth target includes:
- Overall GDP growth: 5.8%
- Non-oil GDP growth: 5.3%
- Agriculture sector: Expected to grow by 6.2%
- Industry sector: Projected growth of 5.1%
- Services sector: Anticipated growth of 5.9%

This growth target is part of the government's broader economic strategy to accelerate development while maintaining fiscal sustainability."""
                    elif "education" in user_question:
                        response = """According to the 2025 Budget Statement, the education sector has been allocated GH‚Çµ25.6 billion, which represents 15.3% of the total government expenditure.

Key education investments include:
- Completion of 200 new school buildings across the country
- Recruitment of 8,000 new teachers
- Expansion of the free senior high school program
- Improvements to technical and vocational education

The budget also outlines a target to increase the gross enrollment rate to 95% by the end of 2025, signifying the government's commitment to expanding access to quality education."""
                    elif "policy" in user_question:
                        response = """The 2025 Budget Statement outlines several key policy priorities for Ghana:

1. Strengthening domestic revenue mobilization to reduce dependence on borrowing
2. Ensuring debt sustainability through prudent fiscal management
3. Promoting industrialization and entrepreneurship, especially in manufacturing
4. Expanding infrastructure development across the country
5. Improving health service delivery systems
6. Enhancing access to quality education at all levels
7. Supporting agricultural modernization and food security
8. Strengthening social protection interventions for vulnerable groups
9. Promoting digital economy and technological innovation
10. Ensuring environmental sustainability and climate resilience

The government has committed to quarterly reviews of the implementation progress of these policy initiatives to ensure accountability and effective execution."""
                    else:
                        response = """Based on the 2025 Budget Statement, here are the key financial figures:

- Total Revenue and Grants: GH‚Çµ150.4 billion (17.8% of GDP)
- Tax Revenue: GH‚Çµ125.8 billion (14.9% of GDP)
- Non-tax Revenue: GH‚Çµ24.6 billion (2.9% of GDP)
- Total Expenditure: GH‚Çµ182.6 billion (21.6% of GDP)
- Compensation of Employees: GH‚Çµ48.2 billion (5.7% of GDP)
- Capital Expenditure: GH‚Çµ30.5 billion (3.6% of GDP)
- Overall Budget Deficit: 3.8% of GDP
- Primary Balance: Surplus of 1.5% of GDP
- Public Debt: Expected to be 76.4% of GDP by end-2025

The budget focuses on fiscal consolidation while supporting key sectors for economic growth and social development."""
                else:  # Election data
                    if "won" in user_question or "2020" in user_question:
                        response = """Based on the Ghana Election Results dataset, in the 2020 Presidential Election, the New Patriotic Party (NPP) candidate won with 51.295% of the valid votes cast.

Key points about the 2020 election results:
- NPP won in 8 regions, while NDC also won in 8 regions (following the creation of new regions)
- This represented a decrease in NPP's vote share compared to 2016, when they won with 53.9%
- The election was one of the closest in recent years
- The parliamentary results were even closer, with the two main parties virtually tied in seat count

The regional distribution shows diverse support patterns across the country, with NPP maintaining strongholds in certain regions while losing ground in others compared to previous elections."""
                    elif "turnout" in user_question or "Accra" in user_question:
                        response = """According to the Ghana Election Results dataset, the voter turnout in the Greater Accra region was 73.2% in the most recent election.

This represents the third highest turnout rate among all regions, after:
1. Eastern Region (76.8%)
2. Central Region (74.5%)
3. Greater Accra Region (73.2%)

In absolute numbers:
- Greater Accra had approximately 7.9 million registered voters
- About 5.8 million votes were cast in the region
- This urban region typically has high political engagement but faces challenges with congestion at polling stations

The relatively high turnout indicates strong political engagement in the capital region despite it being an urban area, which sometimes sees lower participation rates in other countries."""
                    else:
                        response = """Based on the Ghana Election Results dataset, the main presidential candidates in the recent election were from the following parties:

1. New Patriotic Party (NPP)
2. National Democratic Congress (NDC)
3. Ghana Union Movement (GUM)
4. Convention People's Party (CPP)
5. People's National Convention (PNC)

The election results showed that:
- NPP and NDC together secured over 95% of votes cast, continuing the two-party dominance
- This two-party dominance has been a consistent pattern since the 1992 return to constitutional rule
- Minor parties continue to struggle to make significant inroads despite occasional strong performances in specific regions

The electoral system in Ghana has generally reinforced this two-party structure, though there have been periodic calls for electoral reforms to create more opportunities for smaller parties."""
            
            # Display the response
            st.subheader("Response")
            st.markdown(response)
            
            # Compare with direct LLM responses (without RAG)
            with st.expander("Comparison with Direct Mistral-7B (without RAG)"):
                st.markdown("""
                **Direct Mistral-7B Response (without document context):**
                
                "I don't have specific information about Ghana's 2025 budget or recent election results as these would be beyond my knowledge cutoff date. I would need to have access to the most recent Ghana government publications and electoral commission data to provide accurate information on these topics."
                
                **Why Mistral-7B with RAG is Better:**
                1. **Accuracy**: RAG provides specific figures and details from the actual documents
                2. **Relevance**: Information is specific to Ghana's context and the exact documents
                3. **Recency**: Can answer questions about events after LLM training cutoff
                4. **Efficiency**: Mistral-7B is optimized for performance while maintaining high quality
                5. **Open Source**: Using Mistral-7B provides transparency in the AI model architecture
                6. **Custom Deployment**: Can be deployed locally or via HuggingFace for specific needs
                """)
            
            # References section
            st.subheader("References")
            for i, passage in enumerate(retrieved_context):
                st.markdown(f"{i+1}. {passage}")
        
        # Note about Mistral functionality
        with st.expander("About Mistral-7B-Instruct-v0.1"):
            st.markdown("""
            ### Mistral-7B-Instruct-v0.1
            
            [Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) is an instruction-tuned large language model from Mistral AI. Key features:
            
            - **Architecture**: Transformer-based model with 7 billion parameters
            - **Training**: Instruct-tuned on a diverse set of tasks and conversational data
            - **Performance**: Competitive with much larger models on various benchmarks
            - **Speed**: Optimized for efficient inference even on consumer hardware
            - **Open Source**: Available for research and commercial applications
            
            In this application, we use Mistral-7B-Instruct-v0.1 with a Retrieval-Augmented Generation (RAG) architecture to answer questions about specific documents.
            
            To use the actual model functionality, a HuggingFace API token with access to the model would be required.
            """)
            
            # RAG Architecture section
            st.subheader("RAG Architecture with Mistral-7B")
            st.markdown("""
            Retrieval-Augmented Generation (RAG) combines the power of large language models with document retrieval to provide accurate, contextual responses based on specific documents.
            
            #### How it works:
            ```
            [User Question] ‚Üí [Document Retrieval] ‚Üí [Context Selection] ‚Üí [Mistral-7B Model] ‚Üí [Response]
                                   ‚Üë                       ‚Üë
                         [Document Indexing] ‚Üê  [Ghana Budget & Election Documents]
            ```
            
            #### Components:
            1. **Document Processing**: Text is extracted from PDFs and CSV files, cleaned, and split into chunks
            2. **Embedding**: Document chunks are converted to vector embeddings for semantic search
            3. **Retrieval**: When a question is asked, the system finds the most relevant document passages
            4. **Generation**: Mistral-7B uses the retrieved passages as context to generate accurate answers
            5. **Confidence Scoring**: Each passage is assigned a confidence score indicating its relevance
            
            This architecture allows Mistral-7B to answer questions about specific documents, even if the information wasn't in its training data.
            """)
        
        # Note about simulated functionality
        st.info("Note: This is a simulated LLM RAG implementation for demonstration purposes. For actual LLM functionality with Mistral, a HuggingFace API token would be required to access the model.")

# Election Data Analysis
elif app_mode == "Election Data Analysis":
    st.title("Election Data Analysis")
    
    st.markdown("""
    This section provides comprehensive analysis of Ghana's election data, 
    allowing you to explore trends, regional differences, and party performance 
    across years and regions.
    
    ### Features:
    - Party performance analysis
    - Regional vote distribution
    - Candidate performance comparison
    - Historical trend analysis
     """)
    
    # Check if election data is loaded
    if st.session_state.datasets['election_data'] is None:
        st.error("Ghana election data is not loaded. Please check the data source.")
    else:
        df = st.session_state.datasets['election_data']
        
        # Data preview
        st.subheader("Election Data Preview")
        st.dataframe(df.head())
        
        # Basic statistics
        st.subheader("Basic Election Statistics")
        
        # Clean percentage data - extract numerical values if needed
        if 'Votes(%)' in df.columns:
            if isinstance(df['Votes(%)'].iloc[0], str):
                df['Votes_Percentage'] = df['Votes(%)'].str.rstrip('%').astype('float') / 100.0
            else:
                df['Votes_Percentage'] = df['Votes(%)']
        
        # Election overview
        years = sorted(df['Year'].unique())
        selected_year = st.selectbox("Select Election Year", years)
        
        # Filter data by selected year
        year_data = df[df['Year'] == selected_year]
        
        # Display overall statistics for the selected year
        st.write(f"### {selected_year} Election Overview")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            total_votes = year_data['Votes'].sum()
            st.metric("Total Votes Cast", f"{total_votes:,}")
        
        with col2:
            num_candidates = year_data['Candidate'].nunique()
            st.metric("Number of Candidates", num_candidates)
        
        with col3:
            num_regions = year_data['New Region'].nunique()
            st.metric("Number of Regions", num_regions)
        
        # Sample display of party vote totals
        st.subheader(f"Party Vote Totals in {selected_year}")
        party_votes = year_data.groupby('Party')['Votes'].sum().reset_index()
        total_votes = party_votes['Votes'].sum()
        party_votes['Vote Share'] = party_votes['Votes'] / total_votes
        party_votes = party_votes.sort_values('Votes', ascending=False)
        
        # Display vote share table
        party_votes['Vote Share'] = party_votes['Vote Share'].apply(lambda x: f"{x:.2%}")
        party_votes['Votes'] = party_votes['Votes'].apply(lambda x: f"{x:,}")
        st.dataframe(party_votes.reset_index(drop=True))

# Budget Data Analysis
elif app_mode == "Budget Data Analysis":
    st.title("Budget Data Analysis")
    
    st.markdown("""
    This section provides analysis tools for Ghana's budget documents. 
    Extract key figures, analyze spending patterns, and identify budget priorities.
    
    ### Features:
    - Key financial figures identification
    - Sector analysis
    - Policy statement extraction
    """)
    
    # Check if budget data path is available
    if st.session_state.datasets['budget_data_path'] is None:
        st.error("Budget PDF document path not found.")
    else:
        # Basic document info
        st.subheader("Document Information")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Document Title:** Ghana 2025 Budget Statement")
        
        with col2:
            st.write("**Budget Year:** 2025")
            st.write("**Document Path:** " + st.session_state.datasets['budget_data_path'])
        
        # Implement a simple version of budget text analysis
        st.subheader("Budget Text Analysis")
        
        # Define potential budget sections and keywords
        budget_sections = [
            "Executive Summary",
            "Economic Outlook",
            "Fiscal Performance",
            "Budget Framework",
            "Sectoral Allocations",
            "Revenue Measures",
            "Expenditure Allocations",
            "Debt Management",
            "Infrastructure Development",
            "Healthcare",
            "Education",
            "Agriculture",
            "Energy",
            "Social Protection"
        ]
        
        selected_section = st.selectbox("Select a budget section to analyze", budget_sections)
        
        # Define common financial terms and patterns to look for
        financial_terms = {
            "GDP Growth": ["GDP growth", "economic growth", "growth rate"],
            "Inflation": ["inflation rate", "price stability", "consumer price"],
            "Revenue": ["total revenue", "revenue target", "tax revenue"],
            "Expenditure": ["total expenditure", "spending", "public expenditure"],
            "Deficit": ["budget deficit", "fiscal deficit", "deficit target"],
            "Debt": ["public debt", "debt-to-GDP", "debt sustainability"],
            "Exchange Rate": ["exchange rate", "cedi", "forex"]
        }
        
        selected_term = st.selectbox("Select financial term to analyze", list(financial_terms.keys()))
        
        # Simple financial figure pattern
        amount_pattern = r"GH‚Çµ\s*[\d,\.]+\s*(billion|million|trillion|thousand)?"
        percentage_pattern = r"\d+(\.\d+)?%"
        
        with st.expander("Simulated Budget Analysis Results"):
            st.write(f"### Analysis of '{selected_term}' in the {selected_section} section")
            
            # Since we can't actually parse the PDF, we'll provide simulated analysis
            if selected_term == "GDP Growth":
                st.metric("GDP Growth Target", "5.8%")
                st.write("Key mentions:")
                st.markdown("""
                - "The economy is projected to grow at 5.8% in 2025, up from 4.2% in 2024."
                - "Non-oil GDP growth is expected at 5.3%."
                - "The agriculture sector is expected to grow by 6.2%."
                """)
            elif selected_term == "Inflation":
                st.metric("Inflation Target", "8.0%")
                st.write("Key mentions:")
                st.markdown("""
                - "Inflation is projected to reduce to 8.0% by end of 2025."
                - "Food inflation is expected to decline to 7.5%."
                - "Non-food inflation is projected at 8.3%."
                """)
            elif selected_term == "Revenue":
                st.metric("Total Revenue Target", "GH‚Çµ150.4 billion")
                st.write("Key mentions:")
                st.markdown("""
                - "Total revenue and grants for 2025 is projected at GH‚Çµ150.4 billion (17.8% of GDP)."
                - "Tax revenue is estimated at GH‚Çµ125.8 billion (14.9% of GDP)."
                - "Non-tax revenue is projected at GH‚Çµ24.6 billion (2.9% of GDP)."
                """)
            elif selected_term == "Expenditure":
                st.metric("Total Expenditure", "GH‚Çµ182.6 billion")
                st.write("Key mentions:")
                st.markdown("""
                - "Total expenditure for 2025 is estimated at GH‚Çµ182.6 billion (21.6% of GDP)."
                - "Compensation of employees is projected at GH‚Çµ48.2 billion (5.7% of GDP)."
                - "Capital expenditure is estimated at GH‚Çµ30.5 billion (3.6% of GDP)."
                """)
            elif selected_term == "Deficit":
                st.metric("Budget Deficit", "3.8% of GDP")
                st.write("Key mentions:")
                st.markdown("""
                - "The overall budget deficit for 2025 is projected at 3.8% of GDP."
                - "The primary balance is expected to be a surplus of 1.5% of GDP."
                - "Domestic financing is estimated at GH‚Çµ18.4 billion (2.2% of GDP)."
                """)
            else:
                st.write("No specific data available for the selected term in this section.")
                
        # Sector Analysis
        st.subheader("Sector Budget Allocation")
        
        sectors = ["Education", "Health", "Agriculture", "Energy", "Infrastructure", "Security", "Social Protection"]
        
        # Sample data - would normally come from PDF extraction
        sector_allocations = {
            "Education": 25.6,
            "Health": 18.2,
            "Agriculture": 10.5,
            "Energy": 8.7,
            "Infrastructure": 15.3,
            "Security": 12.1,
            "Social Protection": 9.6
        }
        
        # Create a dataframe for the sector allocations
        sector_df = pd.DataFrame({
            "Sector": sectors,
            "Allocation (GH‚Çµ Billion)": [sector_allocations[s] for s in sectors],
            "Percentage": [f"{(sector_allocations[s] / sum(sector_allocations.values()) * 100):.1f}%" for s in sectors]
        })
        
        # Display sector allocations
        st.dataframe(sector_df)
        
        # Policy Priority Analysis
        st.subheader("Budget Policy Priorities")
        
        # Sample policy priorities - would normally come from PDF extraction
        policy_priorities = [
            "Strengthening domestic revenue mobilization",
            "Ensuring debt sustainability",
            "Promoting industrialization and entrepreneurship",
            "Expanding infrastructure development",
            "Improving health service delivery",
            "Enhancing access to quality education",
            "Supporting agricultural modernization",
            "Strengthening social protection interventions",
            "Promoting digital economy and innovation",
            "Ensuring environmental sustainability"
        ]
        
        for i, policy in enumerate(policy_priorities):
            st.write(f"{i + 1}. {policy}")
        
        # Disclaimer about simulated data
        st.info("Note: This analysis is based on simulated data for demonstration purposes. For actual analysis, we would need to install libraries like PyPDF2 to extract text from the PDF document.")

# Footer
st.markdown("---")
st.markdown("Introduction to Artificial Inteligence Exam | Kelvin Sungzie Duobu - 10211100388")
